{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of practice_reinforce.ipynb","provenance":[{"file_id":"https://github.com/yandexdataschool/Practical_RL/blob/coursera/week5_policy_based/practice_reinforce.ipynb","timestamp":1600160922508}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"W5001gAYBUy6","colab_type":"text"},"source":["# REINFORCE in TensorFlow\n","\n","Just like we did before for Q-learning, this time we'll design a TensorFlow network to learn `CartPole-v0` via policy gradient (REINFORCE).\n","\n","Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."]},{"cell_type":"code","metadata":{"id":"f0IAD2wjBUy8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":151},"executionInfo":{"status":"ok","timestamp":1600160995573,"user_tz":-120,"elapsed":18283,"user":{"displayName":"Benoît Hespel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3iSgFVL_-6zHjmzNvEQkCWDFTdJEgdvAchkXw=s64","userId":"00579257794136902113"}},"outputId":"fe8718cd-713d-490f-8ef8-4decd0b4b4d3"},"source":["import sys, os\n","if 'google.colab' in sys.modules:\n","    %tensorflow_version 1.x\n","    \n","    if not os.path.exists('.setup_complete'):\n","        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n","\n","        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n","        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n","\n","        !touch .setup_complete\n","\n","# This code creates a virtual display to draw game images on.\n","# It will have no effect if your machine has a monitor.\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n","    !bash ../xvfb start\n","    os.environ['DISPLAY'] = ':1'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","Selecting previously unselected package xvfb.\n","(Reading database ... 144599 files and directories currently installed.)\n","Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.6_amd64.deb ...\n","Unpacking xvfb (2:1.19.6-1ubuntu4.6) ...\n","Setting up xvfb (2:1.19.6-1ubuntu4.6) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Starting virtual X frame buffer: Xvfb.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kkI8jaCbBUzE","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600160997100,"user_tz":-120,"elapsed":1515,"user":{"displayName":"Benoît Hespel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3iSgFVL_-6zHjmzNvEQkCWDFTdJEgdvAchkXw=s64","userId":"00579257794136902113"}}},"source":["import gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N2iYOOTfBUzJ","colab_type":"text"},"source":["A caveat: we have received reports that the following cell may crash with `NameError: name 'base' is not defined`. The [suggested workaround](https://www.coursera.org/learn/practical-rl/discussions/all/threads/N2Pw652iEemRYQ6W2GuqHg/replies/te3HpQwOQ62tx6UMDoOt2Q/comments/o08gTqelT9KPIE6npX_S3A) is to install `gym==0.14.0` and `pyglet==1.3.2`."]},{"cell_type":"code","metadata":{"id":"6JMMPfU-BUzK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"status":"ok","timestamp":1600161001311,"user_tz":-120,"elapsed":2441,"user":{"displayName":"Benoît Hespel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3iSgFVL_-6zHjmzNvEQkCWDFTdJEgdvAchkXw=s64","userId":"00579257794136902113"}},"outputId":"59dffd27-836f-4655-b766-2d6c270bfc09"},"source":["env = gym.make(\"CartPole-v0\")\n","\n","# gym compatibility: unwrap TimeLimit\n","if hasattr(env, '_max_episode_steps'):\n","    env = env.env\n","\n","env.reset()\n","n_actions = env.action_space.n\n","state_dim = env.observation_space.shape\n","\n","plt.imshow(env.render(\"rgb_array\"))"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f69b58adf28>"]},"metadata":{"tags":[]},"execution_count":3},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"07skqRPSBUzO","colab_type":"text"},"source":["# Building the network for REINFORCE"]},{"cell_type":"markdown","metadata":{"id":"7TZSHvsMBUzP","colab_type":"text"},"source":["For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n","\n","For numerical stability, please __do not include the softmax layer into your network architecture__.\n","We'll use softmax or log-softmax where appropriate."]},{"cell_type":"code","metadata":{"id":"lJn-zqmXBUzQ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600161014650,"user_tz":-120,"elapsed":5515,"user":{"displayName":"Benoît Hespel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3iSgFVL_-6zHjmzNvEQkCWDFTdJEgdvAchkXw=s64","userId":"00579257794136902113"}}},"source":["import tensorflow as tf\n","\n","sess = tf.InteractiveSession()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"P21Cb23BBUzV","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600161022307,"user_tz":-120,"elapsed":1155,"user":{"displayName":"Benoît Hespel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3iSgFVL_-6zHjmzNvEQkCWDFTdJEgdvAchkXw=s64","userId":"00579257794136902113"}}},"source":["# create input variables. We only need <s, a, r> for REINFORCE\n","ph_states = tf.placeholder('float32', (None,) + state_dim, name=\"states\")\n","ph_actions = tf.placeholder('int32', name=\"action_ids\")\n","ph_cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"a7A6hKtCBUzY","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600161100279,"user_tz":-120,"elapsed":1086,"user":{"displayName":"Benoît Hespel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3iSgFVL_-6zHjmzNvEQkCWDFTdJEgdvAchkXw=s64","userId":"00579257794136902113"}}},"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","\n","nn = Sequential()\n","nn.add(Dense(32, activation='relu', input_shape=state_dim))\n","nn.add(Dense(32, activation='relu'))\n","nn.add(Dense(n_actions, activation='linear'))\n","\n","\n","logits = nn(ph_states)\n","\n","policy = tf.nn.softmax(logits)\n","log_policy = tf.nn.log_softmax(logits)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"ssSEI4jiBUzb","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600161104423,"user_tz":-120,"elapsed":1054,"user":{"displayName":"Benoît Hespel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3iSgFVL_-6zHjmzNvEQkCWDFTdJEgdvAchkXw=s64","userId":"00579257794136902113"}}},"source":["# Initialize model parameters\n","sess.run(tf.global_variables_initializer())"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"NY0V8XEwBUze","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600161201999,"user_tz":-120,"elapsed":804,"user":{"displayName":"Benoît Hespel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3iSgFVL_-6zHjmzNvEQkCWDFTdJEgdvAchkXw=s64","userId":"00579257794136902113"}}},"source":["def predict_probs(states):\n","    \"\"\" \n","    Predict action probabilities given states.\n","    :param states: numpy array of shape [batch, state_shape]\n","    :returns: numpy array of shape [batch, n_actions]\n","    \"\"\"\n","    return policy.eval({ph_states: [states]})[0]"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aUAaQWd5BUzh","colab_type":"text"},"source":["### Play the game\n","\n","We can now use our newly built agent to play the game."]},{"cell_type":"code","metadata":{"id":"6q6pHn0PBUzh","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600161377013,"user_tz":-120,"elapsed":1779,"user":{"displayName":"Benoît Hespel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3iSgFVL_-6zHjmzNvEQkCWDFTdJEgdvAchkXw=s64","userId":"00579257794136902113"}}},"source":["def generate_session(env, t_max=1000):\n","    \"\"\" \n","    Play a full session with REINFORCE agent.\n","    Returns sequences of states, actions, and rewards.\n","    \"\"\"\n","    # arrays to record session\n","    states, actions, rewards = [], [], []\n","    s = env.reset()\n","\n","    for t in range(t_max):\n","        # action probabilities array aka pi(a|s)\n","        action_probs = predict_probs(s)\n","\n","        # Sample action with given probabilities.\n","        a = np.random.choice([0,1],p=action_probs)\n","        new_s, r, done, info = env.step(a)\n","\n","        # record session history to train later\n","        states.append(s)\n","        actions.append(a)\n","        rewards.append(r)\n","\n","        s = new_s\n","        if done:\n","            break\n","\n","    return states, actions, rewards"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"x6NsXDI0BUzm","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600161381518,"user_tz":-120,"elapsed":853,"user":{"displayName":"Benoît Hespel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3iSgFVL_-6zHjmzNvEQkCWDFTdJEgdvAchkXw=s64","userId":"00579257794136902113"}}},"source":["# test it\n","states, actions, rewards = generate_session(env)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hWB0-_XwBUzp","colab_type":"text"},"source":["### Computing cumulative rewards\n","\n","$$\n","\\begin{align*}\n","G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n","&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n","&= r_t + \\gamma * G_{t + 1}\n","\\end{align*}\n","$$"]},{"cell_type":"code","metadata":{"id":"Gwzc9N_dBUzq","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600161443575,"user_tz":-120,"elapsed":875,"user":{"displayName":"Benoît Hespel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3iSgFVL_-6zHjmzNvEQkCWDFTdJEgdvAchkXw=s64","userId":"00579257794136902113"}}},"source":["def get_cumulative_rewards(rewards,  # rewards at each step\n","                           gamma=0.99  # discount for reward\n","                           ):\n","    \"\"\"\n","    Take a list of immediate rewards r(s,a) for the whole session \n","    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n","    \n","    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n","\n","    A simple way to compute cumulative rewards is to iterate from the last\n","    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n","\n","    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n","    \"\"\"\n","    cum_rewards = len(rewards) * [0]\n","    cum_rewards[-1] = rewards[-1]\n","    for i in reversed(range(len(rewards)-1)):\n","        cum_rewards[i] = rewards[i] + gamma * cum_rewards[i+1]\n","\n","    return cum_rewards"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"8xqzwtjKBUzt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600161453409,"user_tz":-120,"elapsed":1165,"user":{"displayName":"Benoît Hespel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3iSgFVL_-6zHjmzNvEQkCWDFTdJEgdvAchkXw=s64","userId":"00579257794136902113"}},"outputId":"6f2f88ac-6a56-4a05-823c-a847a9f68bf4"},"source":["assert len(get_cumulative_rewards(range(100))) == 100\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n","    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n","    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n","    [0, 0, 1, 2, 3, 4, 0])\n","print(\"looks good!\")"],"execution_count":15,"outputs":[{"output_type":"stream","text":["looks good!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e4en-oxEBUzv","colab_type":"text"},"source":["#### Loss function and updates\n","\n","We now need to define objective and update over policy gradient.\n","\n","Our objective function is\n","\n","$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n","\n","REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n","\n","$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n","\n","We can abuse Tensorflow's capabilities for automatic differentiation by defining our objective function as follows:\n","\n","$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n","\n","When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."]},{"cell_type":"code","metadata":{"id":"66ouzP6WBUzx","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600161463846,"user_tz":-120,"elapsed":1034,"user":{"displayName":"Benoît Hespel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3iSgFVL_-6zHjmzNvEQkCWDFTdJEgdvAchkXw=s64","userId":"00579257794136902113"}}},"source":["# This code selects the log-probabilities (log pi(a_i|s_i)) for those actions that were actually played.\n","indices = tf.stack([tf.range(tf.shape(log_policy)[0]), ph_actions], axis=-1)\n","log_policy_for_actions = tf.gather_nd(log_policy, indices)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"dUx1zyciBUz4","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600161552345,"user_tz":-120,"elapsed":1076,"user":{"displayName":"Benoît Hespel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3iSgFVL_-6zHjmzNvEQkCWDFTdJEgdvAchkXw=s64","userId":"00579257794136902113"}}},"source":["# Policy objective as in the last formula. Please use reduce_mean, not reduce_sum.\n","# You may use log_policy_for_actions to get log probabilities for actions taken.\n","# Also recall that we defined ph_cumulative_rewards earlier.\n","\n","J = tf.reduce_mean(log_policy_for_actions * ph_cumulative_rewards)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ic4DEEzcBUz7","colab_type":"text"},"source":["As a reminder, for a discrete probability distribution (like the one our policy outputs), entropy is defined as:\n","\n","$$ \\operatorname{entropy}(p) = -\\sum_{i = 1}^n p_i \\cdot \\log p_i $$"]},{"cell_type":"code","metadata":{"id":"tszISo9bBUz8","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600161565638,"user_tz":-120,"elapsed":853,"user":{"displayName":"Benoît Hespel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3iSgFVL_-6zHjmzNvEQkCWDFTdJEgdvAchkXw=s64","userId":"00579257794136902113"}}},"source":["# Entropy regularization. If you don't add it, the policy will quickly deteriorate to\n","# being deterministic, harming exploration.\n","\n","entropy = tf.reduce_sum(policy * log_policy, 1)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"aoYcEJQxBUz-","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600161569189,"user_tz":-120,"elapsed":1036,"user":{"displayName":"Benoît Hespel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3iSgFVL_-6zHjmzNvEQkCWDFTdJEgdvAchkXw=s64","userId":"00579257794136902113"}}},"source":["# # Maximizing X is the same as minimizing -X, hence the sign.\n","loss = -(J + 0.1 * entropy)\n","\n","update = tf.train.AdamOptimizer().minimize(loss)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"_z1HlQndBU0A","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600161571949,"user_tz":-120,"elapsed":1268,"user":{"displayName":"Benoît Hespel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3iSgFVL_-6zHjmzNvEQkCWDFTdJEgdvAchkXw=s64","userId":"00579257794136902113"}}},"source":["def train_on_session(states, actions, rewards, t_max=1000):\n","    \"\"\"given full session, trains agent with policy gradient\"\"\"\n","    cumulative_rewards = get_cumulative_rewards(rewards)\n","    update.run({\n","        ph_states: states,\n","        ph_actions: actions,\n","        ph_cumulative_rewards: cumulative_rewards,\n","    })\n","    return sum(rewards)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"A5vSsQTaBU0D","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600161576329,"user_tz":-120,"elapsed":784,"user":{"displayName":"Benoît Hespel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3iSgFVL_-6zHjmzNvEQkCWDFTdJEgdvAchkXw=s64","userId":"00579257794136902113"}}},"source":["# Initialize optimizer parameters\n","sess.run(tf.global_variables_initializer())"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-xpX4U-mBU0J","colab_type":"text"},"source":["### The actual training"]},{"cell_type":"code","metadata":{"id":"dztzdGJdBU0J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":118},"executionInfo":{"status":"ok","timestamp":1600161634413,"user_tz":-120,"elapsed":54350,"user":{"displayName":"Benoît Hespel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3iSgFVL_-6zHjmzNvEQkCWDFTdJEgdvAchkXw=s64","userId":"00579257794136902113"}},"outputId":"2462d26f-fc89-493b-d161-fc710d7e256a"},"source":["for i in range(100):\n","    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n","\n","    print(\"mean reward: %.3f\" % (np.mean(rewards)))\n","\n","    if np.mean(rewards) > 300:\n","        print(\"You Win!\")  # but you can train even further\n","        break"],"execution_count":23,"outputs":[{"output_type":"stream","text":["mean reward: 30.200\n","mean reward: 89.010\n","mean reward: 215.280\n","mean reward: 223.090\n","mean reward: 446.520\n","You Win!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gPKLXWGfBU0O","colab_type":"text"},"source":["### Results & video"]},{"cell_type":"code","metadata":{"id":"4SmLAkUiBU0O","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600161744042,"user_tz":-120,"elapsed":29231,"user":{"displayName":"Benoît Hespel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3iSgFVL_-6zHjmzNvEQkCWDFTdJEgdvAchkXw=s64","userId":"00579257794136902113"}}},"source":["# Record sessions\n","\n","import gym.wrappers\n","\n","with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n","    sessions = [generate_session(env_monitor) for _ in range(100)]"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dqj8sZrjBU0S","colab_type":"code","colab":{"resources":{"http://localhost:8080/videos/openaigym.video.0.101.video000064.mp4":{"data":"CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K","ok":false,"headers":[["content-length","1449"],["content-type","text/html; charset=utf-8"]],"status":404,"status_text":"Not Found"}},"base_uri":"https://localhost:8080/","height":501},"executionInfo":{"status":"ok","timestamp":1600161754258,"user_tz":-120,"elapsed":1018,"user":{"displayName":"Benoît Hespel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3iSgFVL_-6zHjmzNvEQkCWDFTdJEgdvAchkXw=s64","userId":"00579257794136902113"}},"outputId":"43ab4df1-779c-4ba7-ba49-aa3dd9931377"},"source":["# Show video. This may not work in some setups. If it doesn't\n","# work for you, you can download the videos and view them locally.\n","\n","from pathlib import Path\n","from IPython.display import HTML\n","\n","video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n","\n","HTML(\"\"\"\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"{}\" type=\"video/mp4\">\n","</video>\n","\"\"\".format(video_names[-1]))  # You can also try other indices"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"videos/openaigym.video.0.101.video000064.mp4\" type=\"video/mp4\">\n","</video>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"VfDntQLSBU0W","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1600161952535,"user_tz":-120,"elapsed":39966,"user":{"displayName":"Benoît Hespel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3iSgFVL_-6zHjmzNvEQkCWDFTdJEgdvAchkXw=s64","userId":"00579257794136902113"}},"outputId":"0b111945-b51c-4ad2-b99b-94943cd50f91"},"source":["from submit import submit_cartpole\n","submit_cartpole(generate_session, 'hespel.benoit@gmail.com', '2KaU5ZSMbRJcxyRD')"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Your average reward is 725.36 over 100 episodes\n","Submitted to Coursera platform. See results on assignment page!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FhU32qaZBU0a","colab_type":"text"},"source":["That's all, thank you for your attention!\n","\n","Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."]}]}